{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset of 10000 sequences generated and saved as 'synthetic_dna_dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Function to generate random DNA sequence of a given length\n",
    "def generate_dna_sequence(length=50):\n",
    "    bases = ['A', 'T', 'C', 'G']\n",
    "    return ''.join(random.choices(bases, k=length))\n",
    "\n",
    "# Function to generate synthetic dataset of DNA sequences\n",
    "def generate_dna_dataset(num_sequences=100000, sequence_length=50):\n",
    "    dataset = []\n",
    "    for _ in range(num_sequences):\n",
    "        clean_seq = generate_dna_sequence(sequence_length)\n",
    "        noisy_seq = generate_dna_sequence(sequence_length)\n",
    "        dataset.append([clean_seq, noisy_seq])\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(dataset, columns=['Clean', 'Noisy'])\n",
    "    return df\n",
    "\n",
    "# Generate dataset of 100,000 DNA sequences\n",
    "num_sequences = 10000\n",
    "sequence_length = 50  # Length of each DNA sequence\n",
    "dna_dataset = generate_dna_dataset(num_sequences, sequence_length)\n",
    "\n",
    "# Save the dataset to CSV\n",
    "dna_dataset.to_csv('synthetic_dna_dataset.csv', index=False)\n",
    "print(f\"Dataset of {num_sequences} sequences generated and saved as 'synthetic_dna_dataset.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "     ---------------------------------------- 11.6/11.6 MB 3.4 MB/s eta 0:00:00\n",
      "Collecting numpy>=1.23.2\n",
      "  Downloading numpy-2.2.2-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "     ---------------------------------------- 12.9/12.9 MB 3.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shubh\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "     -------------------------------------- 508.0/508.0 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "     -------------------------------------- 346.6/346.6 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shubh\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.2.2 pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts f2py.exe and numpy-config.exe are installed in 'c:\\Users\\shubh\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.02308432354927063\n",
      "Accuracy: 1258.49%\n",
      "Epoch 2, Loss: 0.021901263928413392\n",
      "Accuracy: 1258.49%\n",
      "Epoch 3, Loss: 0.021827258455753326\n",
      "Accuracy: 1258.49%\n",
      "Epoch 4, Loss: 0.021799881076812744\n",
      "Accuracy: 1258.49%\n",
      "Epoch 5, Loss: 0.02179001553058624\n",
      "Accuracy: 1258.49%\n",
      "Epoch 6, Loss: 0.021784262883663176\n",
      "Accuracy: 1258.49%\n",
      "Epoch 7, Loss: 0.021780766475200653\n",
      "Accuracy: 1258.49%\n",
      "Epoch 8, Loss: 0.021777846717834472\n",
      "Accuracy: 1258.49%\n",
      "Epoch 9, Loss: 0.021775509226322175\n",
      "Accuracy: 1258.49%\n",
      "Epoch 10, Loss: 0.021774122273921965\n",
      "Accuracy: 1258.49%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Positional Encoding (for the Transformer)\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n",
    "\n",
    "# Transformer VAE Model\n",
    "class TransformerVAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, hidden_dim, num_heads, num_layers):\n",
    "        super(TransformerVAE, self).__init__()\n",
    "\n",
    "        # Embedding and Positional Encoding\n",
    "        self.embedding = nn.Embedding(4, hidden_dim)  # 4 DNA bases (A, T, C, G)\n",
    "        self.positional_encoding = PositionalEncoding(hidden_dim)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Latent space mapping (ensure dimensions are correct for the flattened encoder output)\n",
    "        self.to_latent_mu = nn.Linear(hidden_dim * input_dim, latent_dim)\n",
    "        self.to_latent_logvar = nn.Linear(hidden_dim * input_dim, latent_dim)\n",
    "\n",
    "        # Latent to hidden dimension for decoder (after reshaping)\n",
    "        self.latent_to_hidden = nn.Linear(latent_dim, hidden_dim * input_dim)\n",
    "\n",
    "        # Transformer Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=num_heads, batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_dim, 4)  # 4 DNA classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input embedding\n",
    "        embedded = self.embedding(x.long())  # Shape: [batch_size, seq_length, hidden_dim]\n",
    "        embedded = self.positional_encoding(embedded)\n",
    "\n",
    "        # Encoding\n",
    "        encoded = self.encoder(embedded)  # Shape: [batch_size, seq_length, hidden_dim]\n",
    "\n",
    "        # Flatten for latent space (ensure the correct dimension matching)\n",
    "        encoded_flat = encoded.view(x.size(0), -1)  # Flatten to [batch_size, hidden_dim * seq_length]\n",
    "\n",
    "        # Latent space (mean and log-variance)\n",
    "        mu = self.to_latent_mu(encoded_flat)\n",
    "        log_var = self.to_latent_logvar(encoded_flat)\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        z = mu + std * torch.randn_like(std)  # Reparameterization trick\n",
    "\n",
    "        # Latent to hidden dimension (reshaped for decoder)\n",
    "        hidden = self.latent_to_hidden(z).view(x.size(0), x.size(1), -1)\n",
    "\n",
    "        # Decoding\n",
    "        decoded = self.decoder(hidden, encoded)\n",
    "        output = self.output_layer(decoded)  # Predict probabilities for 4 classes\n",
    "\n",
    "        # Apply softmax to get the probabilities\n",
    "        return nn.Softmax(dim=-1)(output), mu, log_var\n",
    "\n",
    "\n",
    "# Function to convert DNA sequence to numeric values\n",
    "def dna_to_numeric(dna_sequence):\n",
    "    mapping = {'A': 0, 'T': 1, 'G': 2, 'C': 3}\n",
    "    return np.array([mapping[base] for base in dna_sequence])\n",
    "\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(predicted, target):\n",
    "    correct_predictions = (predicted == target).sum().item()  # Count the correct bases\n",
    "    accuracy = correct_predictions / target.size(0)  # Divide by the total number of bases\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Function to load synthetic dataset\n",
    "def load_synthetic_dataset(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    clean_sequences = df['Clean'].values\n",
    "    noisy_sequences = df['Noisy'].values\n",
    "    clean_sequences = [dna_to_numeric(seq) for seq in clean_sequences]\n",
    "    noisy_sequences = [dna_to_numeric(seq) for seq in noisy_sequences]\n",
    "\n",
    "    return torch.tensor(np.array(clean_sequences)), torch.tensor(np.array(noisy_sequences))\n",
    "\n",
    "\n",
    "# Model Training Code\n",
    "def train_model(model, train_data, targets, optimizer, criterion):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    output, mu, log_var = model(train_data)\n",
    "\n",
    "    # Calculate loss (using reconstruction loss + KL divergence)\n",
    "    recon_loss = criterion(output.view(-1, 4), targets.view(-1))\n",
    "    kl_divergence = -0.5 * torch.mean(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "\n",
    "    loss = recon_loss + kl_divergence\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "# Model Evaluation Code\n",
    "def evaluate_model(model, test_data, targets):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output, mu, log_var = model(test_data)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predicted_seq = torch.argmax(output, dim=-1)\n",
    "        accuracy = calculate_accuracy(predicted_seq, targets)\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "# Example usage (using synthetic dataset)\n",
    "train_data, train_labels = load_synthetic_dataset('synthetic_dna_dataset.csv')  # Load the dataset\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 32\n",
    "hidden_dim = 64\n",
    "num_heads = 8\n",
    "num_layers = 3\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "# Initialize the model, optimizer, and loss function\n",
    "model = TransformerVAE(input_dim=train_data.shape[1], latent_dim=latent_dim, hidden_dim=hidden_dim, num_heads=num_heads, num_layers=num_layers)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for i in range(0, len(train_data), batch_size):\n",
    "        batch_data = train_data[i:i+batch_size]\n",
    "        batch_labels = train_labels[i:i+batch_size]\n",
    "\n",
    "        loss = train_model(model, batch_data, batch_labels, optimizer, criterion)\n",
    "        epoch_loss += loss\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(train_data)}\")\n",
    "\n",
    "    # Evaluate Model (example after each epoch)\n",
    "    accuracy = evaluate_model(model, train_data, train_labels)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.5.1-cp311-cp311-win_amd64.whl (203.1 MB)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\shubh\\appdata\\roaming\\python\\python311\\site-packages (from torch) (4.12.2)\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 3.3 MB/s eta 0:00:00\n",
      "Collecting jinja2\n",
      "  Downloading jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
      "     -------------------------------------- 134.6/134.6 kB 4.0 MB/s eta 0:00:00\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "     -------------------------------------- 183.9/183.9 kB 3.7 MB/s eta 0:00:00\n",
      "Collecting sympy==1.13.1\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "     ---------------------------------------- 6.2/6.2 MB 4.0 MB/s eta 0:00:00\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "     -------------------------------------- 536.2/536.2 kB 4.8 MB/s eta 0:00:00\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl (15 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch\n",
      "Successfully installed MarkupSafe-3.0.2 filelock-3.16.1 fsspec-2024.12.0 jinja2-3.1.5 mpmath-1.3.0 networkx-3.4.2 sympy-1.13.1 torch-2.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script isympy.exe is installed in 'c:\\Users\\shubh\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts convert-caffe2-to-onnx.exe, convert-onnx-to-caffe2.exe, torchfrtrace.exe and torchrun.exe are installed in 'c:\\Users\\shubh\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
